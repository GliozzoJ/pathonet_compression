{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This nothebook shows how to compute the energy consumption for a given quantized network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-29 17:14:16.226857: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-05-29 17:14:16.226872: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pickle\n",
    "import numpy as np\n",
    "import imageio\n",
    "import json\n",
    "from scipy import misc\n",
    "import timeit\n",
    "import math\n",
    "import os\n",
    "from time_space_eval_pathonet import make_submodel, \\\n",
    "        extract_conv_info, extract_compressed_weights, \\\n",
    "        read_labels, space_calc, product_time, \\\n",
    "        residualDilatedInceptionModule_iMap, PathoNet_iMaps, \\\n",
    "        utils_for_Patho_iMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avoid printing INFO, WARNING, and ERROR messages\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-29 17:14:19.309212: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-29 17:14:19.309420: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-05-29 17:14:19.325341: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2023-05-29 17:14:19.325402: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2023-05-29 17:14:19.325452: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2023-05-29 17:14:19.325496: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2023-05-29 17:14:19.326065: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "#Activate GPU\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-29 17:14:21.394750: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "# Load original model\n",
    "model = tf.keras.models.load_model(\"./original_nets/PathoNet.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this path to use a different compressed model\n",
    "\n",
    "path_quant=\"./pathonet_compressed_models/experiments/UQ_k256/0-0-0-249-0.0001-0.0001-5-75-146.19026.h5\" #CWS_k256/0-0-0-256-0.0001-1e-05-5-75-147.1456.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the considered \"quantized\" model and set weights in original model structure\n",
    "with open(path_quant, \"rb\") as weights:\n",
    "    lw = pickle.load(weights)\n",
    "model.set_weights(lw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load image for testing\n",
    "imageShape = [1228, 1228, 3]\n",
    "    \n",
    "inputShape = [256, 256, 3]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Following, the creation of the PathoNet iMap model and how to load it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import (Input,Add,add,concatenate,Activation,concatenate,\n",
    "                        Concatenate,Dropout,BatchNormalization,Reshape,Permute,\n",
    "                        Dense,UpSampling2D,Flatten,Lambda,Activation,Conv2D,\n",
    "                        DepthwiseConv2D,ZeroPadding2D,GlobalAveragePooling2D,\n",
    "                        MaxPooling2D,AveragePooling2D,LeakyReLU,Conv2DTranspose)\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.utils import get_source_inputs\n",
    "from tensorflow.keras.activations import relu\n",
    "from tensorflow.keras.optimizers import SGD, Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Original Code of PathoNet\n",
    "\n",
    "def residualDilatedInceptionModule(y, nb_channels, _strides=(1, 1),t=\"e\"):\n",
    "    if t==\"d\":\n",
    "        y = Conv2D(nb_channels, kernel_size=(1, 1), strides=(1, 1),kernel_initializer = 'orthogonal',kernel_regularizer= l2(5e-4), padding='same', use_bias=False)(y)\n",
    "        y = BatchNormalization()(y)\n",
    "        y = LeakyReLU()(y)\n",
    "        y = Conv2D(nb_channels, kernel_size=(1, 1), strides=(1, 1),kernel_initializer = 'orthogonal',kernel_regularizer= l2(5e-4), padding='same', use_bias=False)(y)\n",
    "        y = BatchNormalization()(y)\n",
    "        y = LeakyReLU()(y)\n",
    "\n",
    "\n",
    "    A1 = Conv2D(nb_channels, kernel_size=(3, 3), strides=_strides,kernel_initializer = 'orthogonal',kernel_regularizer= l2(5e-4), padding='same', use_bias=False)(y)\n",
    "    A1 = BatchNormalization()(A1)\n",
    "    A1 = LeakyReLU()(A1)\n",
    "    A1 = Conv2D(nb_channels, kernel_size=(3, 3), strides=_strides,kernel_initializer = 'orthogonal',kernel_regularizer= l2(5e-4), padding='same', use_bias=False)(A1)\n",
    "    A1 = BatchNormalization()(A1)\n",
    "    A1 = LeakyReLU()(A1)\n",
    "\n",
    "\n",
    "    A4 = Conv2D(nb_channels, kernel_size=(3, 3), strides=_strides,kernel_initializer = 'orthogonal',kernel_regularizer= l2(5e-4),  dilation_rate=4, padding='same', use_bias=False)(y)\n",
    "    A4 = BatchNormalization()(A4)\n",
    "    A4 = LeakyReLU()(A4)\n",
    "    A4 = Conv2D(nb_channels, kernel_size=(3, 3), strides=_strides,kernel_initializer = 'orthogonal',kernel_regularizer= l2(5e-4),  dilation_rate=4, padding='same', use_bias=False)(A4)\n",
    "    A4 = BatchNormalization()(A4)\n",
    "    A4 = LeakyReLU()(A4)\n",
    "\n",
    "    if (t==\"e\"):\n",
    "        y=concatenate([y,y])\n",
    "    y=add([A1,A4,y])\n",
    "    y = BatchNormalization()(y)\n",
    "    y = LeakyReLU()(y)\n",
    "\n",
    "    return y\n",
    "\n",
    "\n",
    "def PathoNet(input_size = (256,256,3), classes=3, pretrained_weights = None):\n",
    "    inputs = Input(input_size) \n",
    "\n",
    "    block1= Conv2D(16, 3, padding = 'same', kernel_initializer = 'orthogonal',kernel_regularizer= l2(5e-4), use_bias=False)(inputs)\n",
    "    block1 = BatchNormalization()(block1)\n",
    "    block1 = LeakyReLU()(block1)\n",
    "    block1= Conv2D(16, 3, padding = 'same', kernel_initializer = 'orthogonal',kernel_regularizer= l2(5e-4), use_bias=False)(block1)\n",
    "    block1 = BatchNormalization()(block1)\n",
    "    block1 = LeakyReLU()(block1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(block1)\n",
    "\n",
    "    block2= residualDilatedInceptionModule(pool1,32,t=\"e\")\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(block2)\n",
    "\n",
    "    block3= residualDilatedInceptionModule(pool2,64,t=\"e\")\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(block3)\n",
    "\n",
    "    block4= residualDilatedInceptionModule(pool3,128,t=\"e\")\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(block4)\n",
    "    drop4 = Dropout(0.1)(pool4)\n",
    "\n",
    "    block5= residualDilatedInceptionModule(drop4,256,t=\"e\")\n",
    "    drop5 = Dropout(0.1)(block5)\n",
    "\n",
    "    up6 = residualDilatedInceptionModule((UpSampling2D(size = (2,2))(drop5)),128,t=\"d\")\n",
    "    merge6 = concatenate([block4,up6], axis = 3)\n",
    "\n",
    "    up7 = residualDilatedInceptionModule((UpSampling2D(size = (2,2))(merge6)),64,t=\"d\")\n",
    "    merge7 = concatenate([block3,up7], axis = 3)\n",
    "\n",
    "    up8 = residualDilatedInceptionModule((UpSampling2D(size = (2,2))(merge7)),32,t=\"d\")\n",
    "    merge8 = concatenate([block2,up8], axis = 3)\n",
    "\n",
    "    up9 = residualDilatedInceptionModule((UpSampling2D(size = (2,2))(merge8)),16,t=\"d\")\n",
    "    merge9 = concatenate([block1,up9], axis = 3)\n",
    "\n",
    "    block9=Conv2D(16, 3, padding = 'same', kernel_initializer = 'orthogonal',kernel_regularizer= l2(5e-4), use_bias=False)(merge9)\n",
    "    block9 = BatchNormalization()(block9)\n",
    "    block9 = LeakyReLU()(block9)\n",
    "    block9=Conv2D(16, 3, padding = 'same', kernel_initializer = 'orthogonal',kernel_regularizer= l2(5e-4), use_bias=False)(block9)\n",
    "    block9 = BatchNormalization()(block9)\n",
    "    block9 = LeakyReLU()(block9)\n",
    "    block9=Conv2D(8, 3, padding = 'same', kernel_initializer = 'orthogonal',kernel_regularizer= l2(5e-4), use_bias=False)(block9)\n",
    "    block9 = BatchNormalization()(block9)\n",
    "    block9 = LeakyReLU()(block9)\n",
    "    conv10 = Conv2D(classes, 1, activation = 'relu')(block9)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs = inputs, outputs = conv10)\n",
    "\n",
    "    \n",
    "    if(pretrained_weights):\n",
    "        model.load_weights(pretrained_weights)\n",
    "    \n",
    "\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reload weights and set on model created as new\n",
    "\n",
    "with open(path_quant, \"rb\") as weights:\n",
    "    lw = pickle.load(weights)\n",
    "\n",
    "patho = PathoNet(input_size = (256,256,3), classes=3)\n",
    "patho.set_weights(lw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create PathoNet with iMap\n",
    "\n",
    "res_weights, last_bias, indexes_weights, vect_centers = utils_for_Patho_iMap(patho)\n",
    "patho_imap = PathoNet_iMaps(last_bias, indexes_weights, vect_centers, res_weights = res_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1124134/4223457834.py:1: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  img = imageio.imread(\"p9_0219_9.jpg\")\n"
     ]
    }
   ],
   "source": [
    "img = imageio.imread(\"p9_0219_9.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "img=np.expand_dims(np.array(misc.imresize(img,size=(inputShape[0], inputShape[1])))/255, axis=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POWER CONSUMPTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from codecarbon import EmissionsTracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = np.expand_dims(img, axis = 0)\n",
    "img_tmp = img1.squeeze(axis=0)\n",
    "pred = np.array([img_tmp]*500)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POWER CONSUMPTION WITH COMPRESSED MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 17:24:49] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 17:24:49] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 17:24:49] Tracking Nvidia GPU via pynvml\n",
      "[codecarbon INFO @ 17:24:49] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 17:24:49] No CPU tracking mode found. Falling back on CPU constant mode.\n",
      "[codecarbon INFO @ 17:24:50] CPU Model on constant consumption mode: Intel(R) Core(TM) i7-9750HF CPU @ 2.60GHz\n",
      "[codecarbon INFO @ 17:24:50] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 17:24:50]   Platform system: Linux-6.2.6-76060206-generic-x86_64-with-glibc2.35\n",
      "[codecarbon INFO @ 17:24:50]   Python version: 3.10.6\n",
      "[codecarbon INFO @ 17:24:50]   CodeCarbon version: 2.2.2\n",
      "[codecarbon INFO @ 17:24:50]   Available RAM : 15.531 GB\n",
      "[codecarbon INFO @ 17:24:50]   CPU count: 12\n",
      "[codecarbon INFO @ 17:24:50]   CPU model: Intel(R) Core(TM) i7-9750HF CPU @ 2.60GHz\n",
      "[codecarbon INFO @ 17:24:50]   GPU count: 1\n",
      "[codecarbon INFO @ 17:24:50]   GPU model: 1 x NVIDIA GeForce RTX 2060\n",
      "[codecarbon INFO @ 17:25:08] Energy consumed for RAM : 0.000024 kWh. RAM Power : 5.823974132537842 W\n",
      "[codecarbon INFO @ 17:25:08] Energy consumed for all GPUs : 0.000063 kWh. Total GPU Power : 15.188 W\n",
      "[codecarbon INFO @ 17:25:08] Energy consumed for all CPUs : 0.000094 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 17:25:08] 0.000181 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 17:25:23] Energy consumed for RAM : 0.000049 kWh. RAM Power : 5.823974132537842 W\n",
      "[codecarbon INFO @ 17:25:23] Energy consumed for all GPUs : 0.000128 kWh. Total GPU Power : 15.610000000000003 W\n",
      "[codecarbon INFO @ 17:25:23] Energy consumed for all CPUs : 0.000188 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 17:25:23] 0.000365 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 17:25:24] Energy consumed for RAM : 0.000050 kWh. RAM Power : 5.823974132537842 W\n",
      "[codecarbon INFO @ 17:25:24] Energy consumed for all GPUs : 0.000136 kWh. Total GPU Power : 31.78 W\n",
      "[codecarbon INFO @ 17:25:24] Energy consumed for all CPUs : 0.000193 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 17:25:24] 0.000380 kWh of electricity used since the beginning.\n"
     ]
    }
   ],
   "source": [
    "with EmissionsTracker() as tracker:\n",
    "    x = patho_imap.predict(pred, batch_size=50, verbose=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POWER CONSUMPTION WITH ORIGINAL MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 17:25:28] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 17:25:28] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 17:25:28] Tracking Nvidia GPU via pynvml\n",
      "[codecarbon INFO @ 17:25:28] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 17:25:28] No CPU tracking mode found. Falling back on CPU constant mode.\n",
      "[codecarbon INFO @ 17:25:30] CPU Model on constant consumption mode: Intel(R) Core(TM) i7-9750HF CPU @ 2.60GHz\n",
      "[codecarbon INFO @ 17:25:30] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 17:25:30]   Platform system: Linux-6.2.6-76060206-generic-x86_64-with-glibc2.35\n",
      "[codecarbon INFO @ 17:25:30]   Python version: 3.10.6\n",
      "[codecarbon INFO @ 17:25:30]   CodeCarbon version: 2.2.2\n",
      "[codecarbon INFO @ 17:25:30]   Available RAM : 15.531 GB\n",
      "[codecarbon INFO @ 17:25:30]   CPU count: 12\n",
      "[codecarbon INFO @ 17:25:30]   CPU model: Intel(R) Core(TM) i7-9750HF CPU @ 2.60GHz\n",
      "[codecarbon INFO @ 17:25:30]   GPU count: 1\n",
      "[codecarbon INFO @ 17:25:30]   GPU model: 1 x NVIDIA GeForce RTX 2060\n",
      "[codecarbon INFO @ 17:25:48] Energy consumed for RAM : 0.000024 kWh. RAM Power : 5.823974132537842 W\n",
      "[codecarbon INFO @ 17:25:48] Energy consumed for all GPUs : 0.000073 kWh. Total GPU Power : 17.628 W\n",
      "[codecarbon INFO @ 17:25:48] Energy consumed for all CPUs : 0.000094 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 17:25:48] 0.000192 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 17:26:03] Energy consumed for RAM : 0.000049 kWh. RAM Power : 5.823974132537842 W\n",
      "[codecarbon INFO @ 17:26:03] Energy consumed for all GPUs : 0.000179 kWh. Total GPU Power : 25.459 W\n",
      "[codecarbon INFO @ 17:26:03] Energy consumed for all CPUs : 0.000188 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 17:26:03] 0.000416 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 17:26:16] Energy consumed for RAM : 0.000069 kWh. RAM Power : 5.823974132537842 W\n",
      "[codecarbon INFO @ 17:26:16] Energy consumed for all GPUs : 0.000235 kWh. Total GPU Power : 15.318 W\n",
      "[codecarbon INFO @ 17:26:16] Energy consumed for all CPUs : 0.000269 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 17:26:16] 0.000573 kWh of electricity used since the beginning.\n"
     ]
    }
   ],
   "source": [
    "with EmissionsTracker() as tracker:\n",
    "    x = patho.predict(pred, batch_size=50, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
