{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This nothebook shows how to compute the energy consumption for a given quantized network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-29 15:12:00.117680: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-05-29 15:12:00.117730: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pickle\n",
    "import numpy as np\n",
    "import imageio\n",
    "import json\n",
    "from scipy import misc\n",
    "import timeit\n",
    "import math\n",
    "import os\n",
    "from time_space_eval_pathonet import make_submodel, \\\n",
    "        extract_conv_info, extract_compressed_weights, \\\n",
    "        read_labels, space_calc, product_time, \\\n",
    "        residualDilatedInceptionModule_iMap, PathoNet_iMaps, \\\n",
    "        utils_for_Patho_iMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avoid printing INFO, WARNING, and ERROR messages\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Activate GPU\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load original model\n",
    "model = tf.keras.models.load_model(\"./original_nets/PathoNet.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this path to use a different compressed model\n",
    "\n",
    "path_quant=\"./pathonet_compressed_models/experiments/CWS_k256/0-0-0-256-0.0001-1e-05-5-75-147.1456.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the considered \"quantized\" model and set weights in original model structure\n",
    "with open(path_quant, \"rb\") as weights:\n",
    "    lw = pickle.load(weights)\n",
    "model.set_weights(lw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load image for testing\n",
    "imageShape = [1228, 1228, 3]\n",
    "    \n",
    "inputShape = [256, 256, 3]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Following, the creation of the PathoNet iMap model and how to load it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import (Input,Add,add,concatenate,Activation,concatenate,\n",
    "                        Concatenate,Dropout,BatchNormalization,Reshape,Permute,\n",
    "                        Dense,UpSampling2D,Flatten,Lambda,Activation,Conv2D,\n",
    "                        DepthwiseConv2D,ZeroPadding2D,GlobalAveragePooling2D,\n",
    "                        MaxPooling2D,AveragePooling2D,LeakyReLU,Conv2DTranspose)\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.utils import get_source_inputs\n",
    "from tensorflow.keras.activations import relu\n",
    "from tensorflow.keras.optimizers import SGD, Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Original Code of PathoNet\n",
    "\n",
    "def residualDilatedInceptionModule(y, nb_channels, _strides=(1, 1),t=\"e\"):\n",
    "    if t==\"d\":\n",
    "        y = Conv2D(nb_channels, kernel_size=(1, 1), strides=(1, 1),kernel_initializer = 'orthogonal',kernel_regularizer= l2(5e-4), padding='same', use_bias=False)(y)\n",
    "        y = BatchNormalization()(y)\n",
    "        y = LeakyReLU()(y)\n",
    "        y = Conv2D(nb_channels, kernel_size=(1, 1), strides=(1, 1),kernel_initializer = 'orthogonal',kernel_regularizer= l2(5e-4), padding='same', use_bias=False)(y)\n",
    "        y = BatchNormalization()(y)\n",
    "        y = LeakyReLU()(y)\n",
    "\n",
    "\n",
    "    A1 = Conv2D(nb_channels, kernel_size=(3, 3), strides=_strides,kernel_initializer = 'orthogonal',kernel_regularizer= l2(5e-4), padding='same', use_bias=False)(y)\n",
    "    A1 = BatchNormalization()(A1)\n",
    "    A1 = LeakyReLU()(A1)\n",
    "    A1 = Conv2D(nb_channels, kernel_size=(3, 3), strides=_strides,kernel_initializer = 'orthogonal',kernel_regularizer= l2(5e-4), padding='same', use_bias=False)(A1)\n",
    "    A1 = BatchNormalization()(A1)\n",
    "    A1 = LeakyReLU()(A1)\n",
    "\n",
    "\n",
    "    A4 = Conv2D(nb_channels, kernel_size=(3, 3), strides=_strides,kernel_initializer = 'orthogonal',kernel_regularizer= l2(5e-4),  dilation_rate=4, padding='same', use_bias=False)(y)\n",
    "    A4 = BatchNormalization()(A4)\n",
    "    A4 = LeakyReLU()(A4)\n",
    "    A4 = Conv2D(nb_channels, kernel_size=(3, 3), strides=_strides,kernel_initializer = 'orthogonal',kernel_regularizer= l2(5e-4),  dilation_rate=4, padding='same', use_bias=False)(A4)\n",
    "    A4 = BatchNormalization()(A4)\n",
    "    A4 = LeakyReLU()(A4)\n",
    "\n",
    "    if (t==\"e\"):\n",
    "        y=concatenate([y,y])\n",
    "    y=add([A1,A4,y])\n",
    "    y = BatchNormalization()(y)\n",
    "    y = LeakyReLU()(y)\n",
    "\n",
    "    return y\n",
    "\n",
    "\n",
    "def PathoNet(input_size = (256,256,3), classes=3, pretrained_weights = None):\n",
    "    inputs = Input(input_size) \n",
    "\n",
    "    block1= Conv2D(16, 3, padding = 'same', kernel_initializer = 'orthogonal',kernel_regularizer= l2(5e-4), use_bias=False)(inputs)\n",
    "    block1 = BatchNormalization()(block1)\n",
    "    block1 = LeakyReLU()(block1)\n",
    "    block1= Conv2D(16, 3, padding = 'same', kernel_initializer = 'orthogonal',kernel_regularizer= l2(5e-4), use_bias=False)(block1)\n",
    "    block1 = BatchNormalization()(block1)\n",
    "    block1 = LeakyReLU()(block1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(block1)\n",
    "\n",
    "    block2= residualDilatedInceptionModule(pool1,32,t=\"e\")\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(block2)\n",
    "\n",
    "    block3= residualDilatedInceptionModule(pool2,64,t=\"e\")\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(block3)\n",
    "\n",
    "    block4= residualDilatedInceptionModule(pool3,128,t=\"e\")\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(block4)\n",
    "    drop4 = Dropout(0.1)(pool4)\n",
    "\n",
    "    block5= residualDilatedInceptionModule(drop4,256,t=\"e\")\n",
    "    drop5 = Dropout(0.1)(block5)\n",
    "\n",
    "    up6 = residualDilatedInceptionModule((UpSampling2D(size = (2,2))(drop5)),128,t=\"d\")\n",
    "    merge6 = concatenate([block4,up6], axis = 3)\n",
    "\n",
    "    up7 = residualDilatedInceptionModule((UpSampling2D(size = (2,2))(merge6)),64,t=\"d\")\n",
    "    merge7 = concatenate([block3,up7], axis = 3)\n",
    "\n",
    "    up8 = residualDilatedInceptionModule((UpSampling2D(size = (2,2))(merge7)),32,t=\"d\")\n",
    "    merge8 = concatenate([block2,up8], axis = 3)\n",
    "\n",
    "    up9 = residualDilatedInceptionModule((UpSampling2D(size = (2,2))(merge8)),16,t=\"d\")\n",
    "    merge9 = concatenate([block1,up9], axis = 3)\n",
    "\n",
    "    block9=Conv2D(16, 3, padding = 'same', kernel_initializer = 'orthogonal',kernel_regularizer= l2(5e-4), use_bias=False)(merge9)\n",
    "    block9 = BatchNormalization()(block9)\n",
    "    block9 = LeakyReLU()(block9)\n",
    "    block9=Conv2D(16, 3, padding = 'same', kernel_initializer = 'orthogonal',kernel_regularizer= l2(5e-4), use_bias=False)(block9)\n",
    "    block9 = BatchNormalization()(block9)\n",
    "    block9 = LeakyReLU()(block9)\n",
    "    block9=Conv2D(8, 3, padding = 'same', kernel_initializer = 'orthogonal',kernel_regularizer= l2(5e-4), use_bias=False)(block9)\n",
    "    block9 = BatchNormalization()(block9)\n",
    "    block9 = LeakyReLU()(block9)\n",
    "    conv10 = Conv2D(classes, 1, activation = 'relu')(block9)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs = inputs, outputs = conv10)\n",
    "\n",
    "    \n",
    "    if(pretrained_weights):\n",
    "        model.load_weights(pretrained_weights)\n",
    "    \n",
    "\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reload weights and set on model created as new\n",
    "\n",
    "with open(path_quant, \"rb\") as weights:\n",
    "    lw = pickle.load(weights)\n",
    "\n",
    "patho = PathoNet(input_size = (256,256,3), classes=3)\n",
    "patho.set_weights(lw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create PathoNet with iMap\n",
    "\n",
    "res_weights, last_bias, indexes_weights, vect_centers = utils_for_Patho_iMap(patho)\n",
    "patho_imap = PathoNet_iMaps(last_bias, indexes_weights, vect_centers, res_weights = res_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = imageio.imread(\"p9_0219_9.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "img=np.expand_dims(np.array(misc.imresize(img,size=(inputShape[0], inputShape[1])))/255, axis=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POWER CONSUMPTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from codecarbon import EmissionsTracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = np.expand_dims(img, axis = 0)\n",
    "img_tmp = img1.squeeze(axis=0)\n",
    "pred = np.array([img_tmp]*500)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POWER CONSUMPTION WITH COMPRESSED MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 15:27:24] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 15:27:24] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 15:27:24] Tracking Nvidia GPU via pynvml\n",
      "[codecarbon INFO @ 15:27:24] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 15:27:24] No CPU tracking mode found. Falling back on CPU constant mode.\n",
      "[codecarbon INFO @ 15:27:25] CPU Model on constant consumption mode: Intel(R) Core(TM) i7-9750HF CPU @ 2.60GHz\n",
      "[codecarbon INFO @ 15:27:25] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 15:27:25]   Platform system: Linux-6.2.6-76060206-generic-x86_64-with-glibc2.35\n",
      "[codecarbon INFO @ 15:27:25]   Python version: 3.10.6\n",
      "[codecarbon INFO @ 15:27:25]   CodeCarbon version: 2.2.2\n",
      "[codecarbon INFO @ 15:27:25]   Available RAM : 15.531 GB\n",
      "[codecarbon INFO @ 15:27:25]   CPU count: 12\n",
      "[codecarbon INFO @ 15:27:25]   CPU model: Intel(R) Core(TM) i7-9750HF CPU @ 2.60GHz\n",
      "[codecarbon INFO @ 15:27:25]   GPU count: 1\n",
      "[codecarbon INFO @ 15:27:25]   GPU model: 1 x NVIDIA GeForce RTX 2060\n",
      "[codecarbon INFO @ 15:27:44] Energy consumed for RAM : 0.000024 kWh. RAM Power : 5.823974132537842 W\n",
      "[codecarbon INFO @ 15:27:44] Energy consumed for all GPUs : 0.000078 kWh. Total GPU Power : 18.618000000000002 W\n",
      "[codecarbon INFO @ 15:27:44] Energy consumed for all CPUs : 0.000094 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:27:44] 0.000196 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:27:59] Energy consumed for RAM : 0.000049 kWh. RAM Power : 5.823974132537842 W\n",
      "[codecarbon INFO @ 15:27:59] Energy consumed for all GPUs : 0.000161 kWh. Total GPU Power : 20.057000000000002 W\n",
      "[codecarbon INFO @ 15:27:59] Energy consumed for all CPUs : 0.000188 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:27:59] 0.000397 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:28:05] Energy consumed for RAM : 0.000059 kWh. RAM Power : 5.823974132537842 W\n",
      "[codecarbon INFO @ 15:28:05] Energy consumed for all GPUs : 0.000211 kWh. Total GPU Power : 28.584000000000003 W\n",
      "[codecarbon INFO @ 15:28:05] Energy consumed for all CPUs : 0.000227 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:28:05] 0.000497 kWh of electricity used since the beginning.\n"
     ]
    }
   ],
   "source": [
    "with EmissionsTracker() as tracker:\n",
    "    x = patho_imap.predict(pred, batch_size=50, verbose=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POWER CONSUMPTION WITH ORIGINAL MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 15:29:40] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 15:29:40] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 15:29:40] Tracking Nvidia GPU via pynvml\n",
      "[codecarbon INFO @ 15:29:40] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 15:29:40] No CPU tracking mode found. Falling back on CPU constant mode.\n",
      "[codecarbon INFO @ 15:29:42] CPU Model on constant consumption mode: Intel(R) Core(TM) i7-9750HF CPU @ 2.60GHz\n",
      "[codecarbon INFO @ 15:29:42] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 15:29:42]   Platform system: Linux-6.2.6-76060206-generic-x86_64-with-glibc2.35\n",
      "[codecarbon INFO @ 15:29:42]   Python version: 3.10.6\n",
      "[codecarbon INFO @ 15:29:42]   CodeCarbon version: 2.2.2\n",
      "[codecarbon INFO @ 15:29:42]   Available RAM : 15.531 GB\n",
      "[codecarbon INFO @ 15:29:42]   CPU count: 12\n",
      "[codecarbon INFO @ 15:29:42]   CPU model: Intel(R) Core(TM) i7-9750HF CPU @ 2.60GHz\n",
      "[codecarbon INFO @ 15:29:42]   GPU count: 1\n",
      "[codecarbon INFO @ 15:29:42]   GPU model: 1 x NVIDIA GeForce RTX 2060\n",
      "[codecarbon INFO @ 15:30:00] Energy consumed for RAM : 0.000024 kWh. RAM Power : 5.823974132537842 W\n",
      "[codecarbon INFO @ 15:30:00] Energy consumed for all GPUs : 0.000084 kWh. Total GPU Power : 20.117000000000004 W\n",
      "[codecarbon INFO @ 15:30:00] Energy consumed for all CPUs : 0.000094 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:30:00] 0.000202 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:30:15] Energy consumed for RAM : 0.000049 kWh. RAM Power : 5.823974132537842 W\n",
      "[codecarbon INFO @ 15:30:15] Energy consumed for all GPUs : 0.000233 kWh. Total GPU Power : 35.82 W\n",
      "[codecarbon INFO @ 15:30:15] Energy consumed for all CPUs : 0.000188 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:30:15] 0.000469 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:30:30] Energy consumed for RAM : 0.000073 kWh. RAM Power : 5.823974132537842 W\n",
      "[codecarbon INFO @ 15:30:30] Energy consumed for all GPUs : 0.000325 kWh. Total GPU Power : 22.059 W\n",
      "[codecarbon INFO @ 15:30:30] Energy consumed for all CPUs : 0.000281 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:30:30] 0.000679 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:30:35] Energy consumed for RAM : 0.000082 kWh. RAM Power : 5.823974132537842 W\n",
      "[codecarbon INFO @ 15:30:35] Energy consumed for all GPUs : 0.000382 kWh. Total GPU Power : 37.837 W\n",
      "[codecarbon INFO @ 15:30:35] Energy consumed for all CPUs : 0.000315 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:30:35] 0.000778 kWh of electricity used since the beginning.\n"
     ]
    }
   ],
   "source": [
    "with EmissionsTracker() as tracker:\n",
    "    x = patho.predict(pred, batch_size=50, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
